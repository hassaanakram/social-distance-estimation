{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Social Distance Analyzer.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QXrVkDiU2sxcNNBWtV_By366GJzJlMON","authorship_tag":"ABX9TyPffa8j/2qdGjMa2ZH5YlKr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"47N0BLSqVwOH"},"source":["### Change the path according to your google drive directory"]},{"cell_type":"code","metadata":{"id":"7HxIInQ9Sv63","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609307164896,"user_tz":-300,"elapsed":976,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}},"outputId":"5b2c74e6-c17f-4ab9-84dc-a82509f3b247"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/Upwork/Social distance"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Upwork/Social distance\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lkbzCG2bOAus","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609307167506,"user_tz":-300,"elapsed":1070,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}},"outputId":"bdc51e18-dc3b-400b-c144-77237f3f1b3b"},"source":["# Clonning the YOLOv4 git repo (run only once)\r\n","!git clone https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3"],"execution_count":24,"outputs":[{"output_type":"stream","text":["fatal: destination path 'TensorFlow-2.x-YOLOv3' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KbJOUX1AV8Fe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609304393605,"user_tz":-300,"elapsed":1155,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}},"outputId":"3b730c69-bf1a-4127-b3ff-c0ca6eba7702"},"source":["%cd TensorFlow-2.x-YOLOv3"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Upwork/Social distance/TensorFlow-2.x-YOLOv3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V8BTtLWTB3Dw","executionInfo":{"status":"ok","timestamp":1609307176191,"user_tz":-300,"elapsed":997,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}}},"source":["!git config --global user.email \"hassaan.akram.ha@gmail.com\"\r\n","!git config --global user.name \"hassaanakram\"\r\n","#!git config http.postBuffer 524288000"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3u1uMTyWBFnm","executionInfo":{"status":"ok","timestamp":1609306295991,"user_tz":-300,"elapsed":41343,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}},"outputId":"4e3e1665-35aa-48c1-f772-76070803234a"},"source":["!git init\r\n","!git add .\r\n","!git commit -m \"Adding modified files from the original TensorFlow-2.x-YOLOv3 repo\"\r\n","#!git remote add origin https://hassaanakram:Alwaysthunder2_@github.com/hassaanakram/social-distance-estimation.git\r\n","!git push -u origin master"],"execution_count":21,"outputs":[{"output_type":"stream","text":["On branch master\n","nothing to commit, working tree clean\n","Counting objects: 25, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (24/24), done.\n","Writing objects: 100% (25/25), 245.54 MiB | 10.19 MiB/s, done.\n","Total 25 (delta 2), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (2/2), done.\u001b[K\n","remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n","remote: error: Trace: 7d84d9b7d2e7b5dc504085c62723f15bf5cce32d3bc00e424bf6d9ceefd3e306\u001b[K\n","remote: error: See http://git.io/iEPt8g for more information.\u001b[K\n","remote: error: File model_data/yolov4.weights is 245.78 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n","To https://github.com/hassaanakram/social-distance-estimation.git\n"," ! [remote rejected] master -> master (pre-receive hook declined)\n","error: failed to push some refs to 'https://hassaanakram:Alwaysthunder2_@github.com/hassaanakram/social-distance-estimation.git'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bUYSmO-MceEd"},"source":["# To download YOLOv4 weights (run only once)\r\n","!wget -P model_data https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8PSfy52Sk_M"},"source":["import sys\r\n","# Change it according to your system\r\n","repo_path = '/content/drive/MyDrive/Colab Notebooks/Upwork/Social distance/TensorFlow-2.x-YOLOv3'\r\n","sys.path.insert(1, repo_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzagZOx_TCvE"},"source":["# Installing some requirements\r\n","!pip install tqdm==4.43.0\r\n","!pip install awscli\r\n","!pip install urllib3\r\n","!pip install mss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-ONlX-WWQ3A","executionInfo":{"status":"ok","timestamp":1609304075606,"user_tz":-300,"elapsed":6143,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}}},"source":["# Importing dependencies\r\n","import tensorflow as tf\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import os\r\n","import cv2\r\n","from google.colab.patches import cv2_imshow\r\n","from scipy.spatial import distance\r\n","import itertools"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fS8-Vxk_3oP","executionInfo":{"status":"ok","timestamp":1609304084963,"user_tz":-300,"elapsed":1598,"user":{"displayName":"Muhammad Hassaan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnBHdRvV5F-uZMd4JkgMUjJ3k2M2hNF-Y1BauW=s64","userId":"03702731417922112339"}},"outputId":"4d134211-61d7-47eb-aeec-43af2423397e"},"source":["print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7d9H3dMtYoED"},"source":["# YOLOv4 Imports (ignore the v3 name in folder structure)\r\n","from yolov3.utils import Load_Yolo_model, image_preprocess, postprocess_boxes, nms, draw_bbox, read_class_names\r\n","from yolov3.configs import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Niic482rYx21"},"source":["# Some helper functions\r\n","def compute_midpoint(p1, p2):\r\n","  '''Helper function to compute midpoint of two points\r\n","     Where each point is a tuple of integers\r\n","     Returns:\r\n","            list(p1[0]+p2[0]/2, p1[1]+p2[1]/2)\r\n","  '''\r\n","  mpX = (p1[0]+p2[0])/2\r\n","  mpY = (p1[1]+p2[1])/2\r\n","  return np.array([mpX, mpY], dtype=np.int16)\r\n","\r\n","def compute_centroid(box):\r\n","  '''Helper function to compute centroid of box.\r\n","     Arguments:\r\n","            box: 1x4 np array of form:\r\n","                 box[0]: x1, box[1]: y1\r\n","                 box[2]: x2, box[3]: y2\r\n","     Returns: \r\n","           nparray(centroidX, centroidY)\r\n","  '''\r\n","  return compute_midpoint((box[0], box[1]),(box[2], box[3]))\r\n","\r\n","def convert_wh(box):\r\n","  '''Helper function to convert bbox to the width/height form:\r\n","     [width, height, centre]\r\n","     Arguments:\r\n","              box: 1x4 np array of form:\r\n","                   box[0]: x1, box[1]: y1\r\n","                   box[2]: x2, box[3]: y2\r\n","     Returns:\r\n","            nparray([width, height, centre])\r\n","  '''\r\n","  width = box[2]-box[0]\r\n","  height = box[3]-box[1]\r\n","  centre = compute_centroid(box)\r\n","  return np.array([width, height, centre], dtype=object)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBRdR_hya2uj"},"source":["# Process input function\r\n","def get_bboxes(Yolo, frame, input_size, score_threshold, iou_threshold):\r\n","  '''Process the input video and return bboxes in two point and w, h, c form\r\n","     frame: nd array of shape (height, width, channels)\r\n","  Returns:\r\n","     bboxes: list of bouding boxes in two point form\r\n","     bboxes_centroidForm: list of bboxes in w, h, c form\r\n","  '''\r\n","  try:\r\n","    # Change colour space\r\n","    original_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n","    original_frame = cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB)\r\n","  except:\r\n","    print('Exception while converting colour space. Returning')\r\n","    return\r\n","    \r\n","  # Preprocess as per YOLO's requirements\r\n","  image_data = image_preprocess(np.copy(original_frame),\r\n","                                [input_size, input_size])\r\n","  image_data = image_data[np.newaxis,...].astype(np.float32)\r\n","\r\n","  # Get predictions\r\n","  pred_bbox = Yolo.predict(image_data)\r\n","\r\n","  # Process the pred_bbox\r\n","  pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\r\n","  pred_bbox = tf.concat(pred_bbox, axis=0)\r\n","  bboxes = postprocess_boxes(pred_bbox, original_frame, input_size,\r\n","                             score_threshold)\r\n","  bboxes = nms(bboxes, iou_threshold, method='nms')\r\n","  scores = list(map(lambda x: x[4], bboxes))\r\n","  # Get width,heigth,centroid form\r\n","  bboxes_centroidForm = list(map(convert_wh, bboxes))\r\n","  \r\n","  return bboxes, bboxes_centroidForm, scores\r\n","\r\n","def get_ppi(bboxes_whc):\r\n","  '''get_ppi function returns the pixels per inch for the reference object.\r\n","     Arguments:\r\n","              bboxes_whc: bounding boxes list in width,height,centroid form\r\n","     Returns:\r\n","              ppi: pixels per inch\r\n","  '''\r\n","  approx_width_inches = 25\r\n","  # Identify the central detection\r\n","\r\n","  # Get X coords of all detected centres, sort, and get the middle one\r\n","  centroids_x = list(map(lambda x: x[2][0], bboxes_whc))\r\n","  centroids_xSorted = sorted(centroids_x)\r\n","  print(int(len(centroids_x)/2))\r\n","  middle = centroids_xSorted[int(len(centroids_x)/2)]\r\n","  idx_middle = list(map(lambda x: np.nonzero(x==middle), centroids_x))\r\n","  idx_middle = list(map(lambda x: x[0].size==0, idx_middle))\r\n","  idx_middle = idx_middle.index(0)\r\n","\r\n","  width_middle = bboxes_whc[idx_middle][0]\r\n","  return width_middle/approx_width_inches\r\n","\r\n","\r\n","def compute_distances(bboxes_whc, ppi):\r\n","  '''To calculate distance between all detected persons without repetitions\r\n","     Arguments:\r\n","              bboxes_whc: bounding boxes of form (width, height, centroid)\r\n","     Returns:\r\n","              distances = list(all distance)\r\n","              combinations\r\n","              centroids\r\n","  '''\r\n","  distances = []\r\n","  centroids = list(map(lambda x: tuple(x[2]), bboxes_whc))\r\n","\r\n","  # Combinations\r\n","  list_combinations = list(itertools.combinations(centroids, 2))\r\n","  \r\n","  for i, pair in enumerate(list_combinations):\r\n","    distances.append(distance.euclidean(pair[0], pair[1])/ppi)\r\n","  return distances, list(itertools.combinations(list(range(len(centroids))), 2)), centroids\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFx191onU2XD"},"source":["def process_input(input_video, output_path, fps):\r\n","  '''Process input function to perform detectios, overlays\r\n","  '''\r\n","   # Set some detection parameters\r\n","  Track_only = [\"person\"]\r\n","  rectangle_colors = ''\r\n","  iou_threshold = 0.1\r\n","  score_threshold = 0.3\r\n","  CLASSES = YOLO_COCO_CLASSES\r\n","  show = False\r\n","  input_size = 416\r\n","   # Number of frames \r\n","  N = input_video.shape[0]\r\n","\r\n","  # Get frame dimensions and set codec info\r\n","  width = input_video.shape[2]\r\n","  height = input_video.shape[1]\r\n","  codec = cv2.VideoWriter_fourcc(*'XVID')\r\n","  out = cv2.VideoWriter(output_path, codec, fps,\r\n","                        (width,height))\r\n","  NUM_CLASS  = read_class_names(CLASSES)\r\n","  key_list = list(NUM_CLASS.keys())\r\n","  val_list = list(NUM_CLASS.values())\r\n","  yolo = Load_Yolo_model()\r\n","  # Start reading frames\r\n","  for i in range(N):\r\n","    frame = input_video[i, :,:,:]\r\n","    bboxes, bboxes_whc, scores = get_bboxes(yolo, frame, input_size, score_threshold,\r\n","                                        iou_threshold)\r\n","    if len(bboxes) == 0:\r\n","      continue\r\n","    ppi = get_ppi(bboxes_whc)\r\n","    distances, combinations, centroids = compute_distances(bboxes_whc, ppi)\r\n","\r\n","    # Overlay bouding boxes and distance lines\r\n","    for i in range(len(bboxes)):\r\n","      bbox = bboxes[i]\r\n","      bbox = np.array(bbox[:4], dtype=np.int32)\r\n","      cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255,255,0), 2)\r\n","      label = \" {:.2f}\".format(scores[i])\r\n","      # get text size\r\n","      (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\r\n","                                                2*0.75, thickness=2)\r\n","      # put filled text rectangle\r\n","      cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[0] + text_width, bbox[1] - text_height - baseline), (255,255,0), thickness=cv2.FILLED)\r\n","\r\n","      # put text above rectangle\r\n","      cv2.putText(frame, label, (bbox[0], bbox[1]-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\r\n","                  0.75*2, (255,255,255), 2, lineType=cv2.LINE_AA)\r\n","                        \r\n","    for i, pair in enumerate(combinations):\r\n","      if distances[i] < 60:\r\n","        color = (0,0,255)\r\n","      else:\r\n","        color = (0,255,0)\r\n","\r\n","      # Get coords of corresponding persons\r\n","      person1 = centroids[pair[0]]\r\n","      person2 = centroids[pair[1]]\r\n","\r\n","      # Draw line\r\n","      cv2.line(frame, person1, person2, color, 1)\r\n","    \r\n","    out.write(frame)\r\n","  \r\n","  out.release()\r\n","  cv2.destroyAllWindows()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ei9bAI4WWRSL"},"source":["### Change the filepath in VideoCapture to your video"]},{"cell_type":"code","metadata":{"id":"ZddmV9gaayYn"},"source":["# Test bench\r\n","cap = cv2.VideoCapture('/content/drive/MyDrive/Colab Notebooks/Upwork/Social distance/v2.mp4')\r\n","frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n","frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\r\n","frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n","\r\n","buf = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\r\n","fc = 0\r\n","ret = True\r\n","\r\n","while (fc < frameCount  and ret):\r\n","    ret, buf[fc] = cap.read()\r\n","    fc += 1\r\n","\r\n","cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OcACwVksWWDT"},"source":["### Set the output path in the second argument of process_input. Make sure that it ends in .mp4"]},{"cell_type":"code","metadata":{"id":"3u3LqXcna46f"},"source":["process_input(buf, '/content/drive/MyDrive/Colab Notebooks/Upwork/Social distance/out13.mp4', fps)"],"execution_count":null,"outputs":[]}]}